{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['image0.jpg', 'image1.jpg']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "\n",
    "#crop image to middle\n",
    "image_width = 500\n",
    "image_height = 500\n",
    "input_path = \"input_photos\"\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "included_extensions = ['jpg', 'bmp', 'png', 'gif']\n",
    "onlyfiles = [fn for fn in os.listdir(input_path)\n",
    "              if any(fn.endswith(ext) for ext in included_extensions)]\n",
    "\n",
    "onlyfiles[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from numpy import*\n",
    "\n",
    "frames = []\n",
    "def newFrame(cur_filepath):\n",
    "    temp=Image.open(cur_filepath)\n",
    "    temp=temp.convert('1')      # Convert to black&white\n",
    "    A = array(temp)             # Creates an array, white pixels==True and black pixels==False\n",
    "    new_A=empty((A.shape[0],A.shape[1]),None)    #New array with same size as A\n",
    "\n",
    "    for i in range(len(A)):\n",
    "        for j in range(len(A[i])):\n",
    "            if A[i][j]==True:\n",
    "                new_A[i][j]=0\n",
    "            else:\n",
    "                new_A[i][j]=1\n",
    "    frames.append(new_A)\n",
    "\n",
    "for x in onlyfiles:\n",
    "    newFrame(input_path + \"/\" + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = np.array(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "720"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(frames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1280"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(frames[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 720, 1280)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 720, 1280, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(frames, axis=3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = []\n",
    "x1 = []\n",
    "x2 = []\n",
    "def middle_out(frame1, frame2, frame3):\n",
    "    \n",
    "    target.append(frame2)\n",
    "    x1.append(frame1)\n",
    "    x2.append(frame3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_out_frames(cur_scene):\n",
    "    for idx in range(0,len(cur_scene)-2):\n",
    "        middle_out(cur_scene[idx], cur_scene[idx + 1], cur_scene[idx + 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pick_out_frames(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73, 720, 1280)\n",
      "(73, 720, 1280)\n",
      "(73, 720, 1280)\n"
     ]
    }
   ],
   "source": [
    "target = np.array(target)\n",
    "x1 = np.array(x1)\n",
    "x2 = np.array(x2)\n",
    "\n",
    "print(target.shape)\n",
    "print(x1.shape)\n",
    "print(x2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73, 720, 1280, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack((x1,x2), axis=3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.stack((x1,x2), axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from voxel_flow_model import Voxel_flow_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dimension size must be evenly divisible by 1843200 but is 4194304 for 'interpolate/Reshape_5' (op: 'Reshape') with input shapes: [2097152,2], [4] and with input tensors computed as partial shapes: input[1] = [?,720,1280,2].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mD:\\Programs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1575\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1576\u001b[1;33m     \u001b[0mc_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1577\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Dimension size must be evenly divisible by 1843200 but is 4194304 for 'interpolate/Reshape_5' (op: 'Reshape') with input shapes: [2097152,2], [4] and with input tensors computed as partial shapes: input[1] = [?,720,1280,2].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-3d1b1aaa3b0f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVoxel_flow_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_placeholder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m# reproduction_loss, prior_loss = model.loss(prediction, target_placeholder)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Hackthenorth\\middle_out_backend\\voxel_flow_model.py\u001b[0m in \u001b[0;36minference\u001b[1;34m(self, input_images)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mArgs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \"\"\"\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Hackthenorth\\middle_out_backend\\voxel_flow_model.py\u001b[0m in \u001b[0;36m_build_model\u001b[1;34m(self, input_images)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     output_1 = bilinear_interp(\n\u001b[1;32m---> 87\u001b[1;33m         input_images[:, :, :, 0:3], coor_x_1, coor_y_1, 'interpolate')\n\u001b[0m\u001b[0;32m     88\u001b[0m     output_2 = bilinear_interp(\n\u001b[0;32m     89\u001b[0m         input_images[:, :, :, 3:6], coor_x_2, coor_y_2, 'interpolate')\n",
      "\u001b[1;32mD:\\Hackthenorth\\middle_out_backend\\utils\\geo_layer_utils.py\u001b[0m in \u001b[0;36mbilinear_interp\u001b[1;34m(im, x, y, name)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchannels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[1;34m(tensor, shape, name)\u001b[0m\n\u001b[0;32m   7432\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0m_ctx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7433\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m-> 7434\u001b[1;33m         \"Reshape\", tensor=tensor, shape=shape, name=name)\n\u001b[0m\u001b[0;32m   7435\u001b[0m     \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7436\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[0;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    788\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    452\u001b[0m                 \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m                 instructions)\n\u001b[1;32m--> 454\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[0;32m    456\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[1;32mD:\\Programs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3153\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3154\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3155\u001b[1;33m           op_def=op_def)\n\u001b[0m\u001b[0;32m   3156\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3157\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   1729\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[0;32m   1730\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[1;32m-> 1731\u001b[1;33m                                 control_input_ops)\n\u001b[0m\u001b[0;32m   1732\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[1;31m# Initialize self._outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1577\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1578\u001b[0m     \u001b[1;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1579\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1581\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Dimension size must be evenly divisible by 1843200 but is 4194304 for 'interpolate/Reshape_5' (op: 'Reshape') with input shapes: [2097152,2], [4] and with input tensors computed as partial shapes: input[1] = [?,720,1280,2]."
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    # Create input and target placeholder.\n",
    "    input_placeholder = tf.placeholder(tf.float32, shape=(None, 256, 256, 2))\n",
    "    target_placeholder = tf.placeholder(tf.float32, shape=(None, 256, 256, 1))\n",
    "\n",
    "    # input_resized = tf.image.resize_area(input_placeholder, [128, 128])\n",
    "    # target_resized = tf.image.resize_area(target_placeholder,[128, 128])\n",
    "\n",
    "    # Prepare model.\n",
    "    model = Voxel_flow_model()\n",
    "    print(\"1\")\n",
    "    prediction = model.inference(input_placeholder)\n",
    "    print(\"2\")\n",
    "    # reproduction_loss, prior_loss = model.loss(prediction, target_placeholder)\n",
    "    reproduction_loss = model.loss(prediction, target_placeholder)\n",
    "    # total_loss = reproduction_loss + prior_loss\n",
    "    total_loss = reproduction_loss\n",
    "\n",
    "    # Perform learning rate scheduling.\n",
    "    learning_rate = FLAGS.initial_learning_rate\n",
    "\n",
    "    # Create an optimizer that performs gradient descent.\n",
    "    opt = tf.train.AdamOptimizer(learning_rate)\n",
    "    grads = opt.compute_gradients(total_loss)\n",
    "    update_op = opt.apply_gradients(grads)\n",
    "\n",
    "    # Create summaries\n",
    "    summaries = tf.get_collection(tf.GraphKeys.SUMMARIES)\n",
    "    summaries.append(tf.scalar_summary('total_loss', total_loss))\n",
    "    summaries.append(tf.scalar_summary('reproduction_loss', reproduction_loss))\n",
    "    # summaries.append(tf.scalar_summary('prior_loss', prior_loss))\n",
    "    summaries.append(tf.image_summary('Input Image', input_placeholder, 3))\n",
    "    summaries.append(tf.image_summary('Output Image', prediction, 3))\n",
    "    summaries.append(tf.image_summary('Target Image', target_placeholder, 3))\n",
    "\n",
    "    # Create a saver.\n",
    "    saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "    # Build the summary operation from the last tower summaries.\n",
    "    summary_op = tf.merge_all_summaries()\n",
    "\n",
    "    # Build an initialization operation to run below.\n",
    "    init = tf.initialize_all_variables()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Summary Writter\n",
    "    summary_writer = tf.train.SummaryWriter(\n",
    "        FLAGS.train_dir,\n",
    "        graph=sess.graph)\n",
    "\n",
    "    # Training loop using feed dict method.\n",
    "    data_list_frame1 = dataset_frame1.read_data_list_file()\n",
    "    random.seed(1)\n",
    "    shuffle(data_list_frame1)\n",
    "\n",
    "    data_list_frame2 = dataset_frame2.read_data_list_file()\n",
    "    random.seed(1)\n",
    "    shuffle(data_list_frame2)\n",
    "\n",
    "    data_list_frame3 = dataset_frame3.read_data_list_file()\n",
    "    random.seed(1)\n",
    "    shuffle(data_list_frame3)\n",
    "\n",
    "    data_size = len(data_list_frame1)\n",
    "    epoch_num = int(data_size / FLAGS.batch_size)\n",
    "\n",
    "    # num_workers = 1\n",
    "\n",
    "    # load_fn_frame1 = partial(dataset_frame1.process_func)\n",
    "    # p_queue_frame1 = PrefetchQueue(load_fn_frame1, data_list_frame1, FLAGS.batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    # load_fn_frame2 = partial(dataset_frame2.process_func)\n",
    "    # p_queue_frame2 = PrefetchQueue(load_fn_frame2, data_list_frame2, FLAGS.batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    # load_fn_frame3 = partial(dataset_frame3.process_func)\n",
    "    # p_queue_frame3 = PrefetchQueue(load_fn_frame3, data_list_frame3, FLAGS.batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    for step in xrange(0, FLAGS.max_steps):\n",
    "      batch_idx = step % epoch_num\n",
    "\n",
    "      batch_data_list_frame1 = data_list_frame1[int(\n",
    "          batch_idx * FLAGS.batch_size): int((batch_idx + 1) * FLAGS.batch_size)]\n",
    "      batch_data_list_frame2 = data_list_frame2[int(\n",
    "          batch_idx * FLAGS.batch_size): int((batch_idx + 1) * FLAGS.batch_size)]\n",
    "      batch_data_list_frame3 = data_list_frame3[int(\n",
    "          batch_idx * FLAGS.batch_size): int((batch_idx + 1) * FLAGS.batch_size)]\n",
    "\n",
    "      # Load batch data.\n",
    "      batch_data_frame1 = np.array(\n",
    "          [dataset_frame1.process_func(line) for line in batch_data_list_frame1])\n",
    "      batch_data_frame2 = np.array(\n",
    "          [dataset_frame2.process_func(line) for line in batch_data_list_frame2])\n",
    "      batch_data_frame3 = np.array(\n",
    "          [dataset_frame3.process_func(line) for line in batch_data_list_frame3])\n",
    "\n",
    "      # batch_data_frame1 = p_queue_frame1.get_batch()\n",
    "      # batch_data_frame2 = p_queue_frame2.get_batch()\n",
    "      # batch_data_frame3 = p_queue_frame3.get_batch()\n",
    "\n",
    "      feed_dict = {input_placeholder: np.concatenate(\n",
    "          (batch_data_frame1, batch_data_frame3), 3), target_placeholder: batch_data_frame2}\n",
    "\n",
    "      # Run single step update.\n",
    "      _, loss_value = sess.run([update_op, total_loss], feed_dict=feed_dict)\n",
    "\n",
    "      if batch_idx == 0:\n",
    "        # Shuffle data at each epoch.\n",
    "        random.seed(1)\n",
    "        shuffle(data_list_frame1)\n",
    "        random.seed(1)\n",
    "        shuffle(data_list_frame2)\n",
    "        random.seed(1)\n",
    "        shuffle(data_list_frame3)\n",
    "        print('Epoch Number: %d' % int(step / epoch_num))\n",
    "\n",
    "      # Output Summary\n",
    "      if step % 10 == 0:\n",
    "        # summary_str = sess.run(summary_op, feed_dict = feed_dict)\n",
    "        # summary_writer.add_summary(summary_str, step)\n",
    "\t      print(\"Loss at step %d: %f\" % (step, loss_value))\n",
    "\n",
    "      if step % 500 == 0:\n",
    "        # Run a batch of images\n",
    "        prediction_np, target_np = sess.run(\n",
    "            [prediction, target_placeholder], feed_dict=feed_dict)\n",
    "        for i in range(0, prediction_np.shape[0]):\n",
    "          file_name = FLAGS.train_image_dir+str(i)+'_out.png'\n",
    "          file_name_label = FLAGS.train_image_dir+str(i)+'_gt.png'\n",
    "          imwrite(file_name, prediction_np[i, :, :, :])\n",
    "          imwrite(file_name_label, target_np[i, :, :, :])\n",
    "\n",
    "      # Save checkpoint\n",
    "      if step % 5000 == 0 or (step + 1) == FLAGS.max_steps:\n",
    "        checkpoint_path = os.path.join(FLAGS.train_dir, 'model.ckpt')\n",
    "        saver.save(sess, checkpoint_path, global_step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
