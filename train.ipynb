{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['image0.jpg', 'image1.jpg']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "\n",
    "#crop image to middle\n",
    "image_width = 500\n",
    "image_height = 500\n",
    "input_path = \"input_photos\"\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "included_extensions = ['jpg', 'bmp', 'png', 'gif']\n",
    "onlyfiles = [fn for fn in os.listdir(input_path)\n",
    "              if any(fn.endswith(ext) for ext in included_extensions)]\n",
    "\n",
    "onlyfiles[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from numpy import*\n",
    "\n",
    "frames = []\n",
    "def newFrame(cur_filepath):\n",
    "    img = Image.open(cur_filepath).convert('1')\n",
    "    frames.append(array(img.getdata()).reshape(img.size[0], img.size[1], 1))\n",
    "#     temp=temp.convert('1')      # Convert to black&white\n",
    "#     A = array(temp)             # Creates an array, white pixels==True and black pixels==False\n",
    "#     new_A=empty((A.shape[0],A.shape[1]),None)    #New array with same size as A\n",
    "\n",
    "#     for i in range(len(A)):\n",
    "#         for j in range(len(A[i])):\n",
    "#             if A[i][j]==True:\n",
    "#                 new_A[i][j]=0\n",
    "#             else:\n",
    "#                 new_A[i][j]=1\n",
    "#     frames.append(new_A)\n",
    "\n",
    "for x in onlyfiles:\n",
    "    newFrame(input_path + \"/\" + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = np.array(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 720, 1280, 1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(frames, axis=3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = []\n",
    "x1 = []\n",
    "x2 = []\n",
    "def middle_out(frame1, frame2, frame3):\n",
    "    \n",
    "    target.append(frame2)\n",
    "    x1.append(frame1)\n",
    "    x2.append(frame3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_out_frames(cur_scene):\n",
    "    for idx in range(0,len(cur_scene)-2):\n",
    "        middle_out(cur_scene[idx], cur_scene[idx + 1], cur_scene[idx + 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pick_out_frames(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73, 720, 1280)\n",
      "(73, 720, 1280)\n",
      "(73, 720, 1280)\n"
     ]
    }
   ],
   "source": [
    "target = np.array(target)\n",
    "x1 = np.array(x1)\n",
    "x2 = np.array(x2)\n",
    "\n",
    "print(target.shape)\n",
    "print(x1.shape)\n",
    "print(x2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73, 1280, 720, 2, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack((x1,x2), axis=3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.stack((x1,x2), axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from voxel_flow_model import Voxel_flow_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-1251d5c8c2b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVoxel_flow_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_placeholder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m# reproduction_loss, prior_loss = model.loss(prediction, target_placeholder)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Hackthenorth\\middle_out_backend\\voxel_flow_model.py\u001b[0m in \u001b[0;36minference\u001b[1;34m(self, input_images)\u001b[0m\n\u001b[0;32m     20\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0minference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \"\"\"Inference on a set of input_images.\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mArgs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \"\"\"\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Hackthenorth\\middle_out_backend\\voxel_flow_model.py\u001b[0m in \u001b[0;36m_build_model\u001b[1;34m(self, input_images)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[0mcoor_x_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_x\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mflow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[0mcoor_y_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_y\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mflow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;31m# print('hi')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Hackthenorth\\middle_out_backend\\utils\\geo_layer_utils.py\u001b[0m in \u001b[0;36mbilinear_interp\u001b[1;34m(im, x, y, name)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;31m# constants\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mnum_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 3)"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    # Create input and target placeholder.\n",
    "    input_placeholder = tf.placeholder(tf.float32, shape=(None, 256, 256, 2))\n",
    "    target_placeholder = tf.placeholder(tf.float32, shape=(None, 256, 256, 1))\n",
    "\n",
    "    # input_resized = tf.image.resize_area(input_placeholder, [128, 128])\n",
    "    # target_resized = tf.image.resize_area(target_placeholder,[128, 128])\n",
    "\n",
    "    # Prepare model.\n",
    "    model = Voxel_flow_model()\n",
    "    print(\"1\")\n",
    "    prediction = model.inference(input_placeholder)\n",
    "    print(\"2\")\n",
    "    # reproduction_loss, prior_loss = model.loss(prediction, target_placeholder)\n",
    "    reproduction_loss = model.loss(prediction, target_placeholder)\n",
    "    # total_loss = reproduction_loss + prior_loss\n",
    "    total_loss = reproduction_loss\n",
    "\n",
    "    # Perform learning rate scheduling.\n",
    "    learning_rate = FLAGS.initial_learning_rate\n",
    "\n",
    "    # Create an optimizer that performs gradient descent.\n",
    "    opt = tf.train.AdamOptimizer(learning_rate)\n",
    "    grads = opt.compute_gradients(total_loss)\n",
    "    update_op = opt.apply_gradients(grads)\n",
    "\n",
    "    # Create summaries\n",
    "    summaries = tf.get_collection(tf.GraphKeys.SUMMARIES)\n",
    "    summaries.append(tf.scalar_summary('total_loss', total_loss))\n",
    "    summaries.append(tf.scalar_summary('reproduction_loss', reproduction_loss))\n",
    "    # summaries.append(tf.scalar_summary('prior_loss', prior_loss))\n",
    "    summaries.append(tf.image_summary('Input Image', input_placeholder, 3))\n",
    "    summaries.append(tf.image_summary('Output Image', prediction, 3))\n",
    "    summaries.append(tf.image_summary('Target Image', target_placeholder, 3))\n",
    "\n",
    "    # Create a saver.\n",
    "    saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "    # Build the summary operation from the last tower summaries.\n",
    "    summary_op = tf.merge_all_summaries()\n",
    "\n",
    "    # Build an initialization operation to run below.\n",
    "    init = tf.initialize_all_variables()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Summary Writter\n",
    "    summary_writer = tf.train.SummaryWriter(\n",
    "        FLAGS.train_dir,\n",
    "        graph=sess.graph)\n",
    "\n",
    "    # Training loop using feed dict method.\n",
    "    data_list_frame1 = dataset_frame1.read_data_list_file()\n",
    "    random.seed(1)\n",
    "    shuffle(data_list_frame1)\n",
    "\n",
    "    data_list_frame2 = dataset_frame2.read_data_list_file()\n",
    "    random.seed(1)\n",
    "    shuffle(data_list_frame2)\n",
    "\n",
    "    data_list_frame3 = dataset_frame3.read_data_list_file()\n",
    "    random.seed(1)\n",
    "    shuffle(data_list_frame3)\n",
    "\n",
    "    data_size = len(data_list_frame1)\n",
    "    epoch_num = int(data_size / FLAGS.batch_size)\n",
    "\n",
    "    # num_workers = 1\n",
    "\n",
    "    # load_fn_frame1 = partial(dataset_frame1.process_func)\n",
    "    # p_queue_frame1 = PrefetchQueue(load_fn_frame1, data_list_frame1, FLAGS.batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    # load_fn_frame2 = partial(dataset_frame2.process_func)\n",
    "    # p_queue_frame2 = PrefetchQueue(load_fn_frame2, data_list_frame2, FLAGS.batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    # load_fn_frame3 = partial(dataset_frame3.process_func)\n",
    "    # p_queue_frame3 = PrefetchQueue(load_fn_frame3, data_list_frame3, FLAGS.batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    for step in xrange(0, FLAGS.max_steps):\n",
    "      batch_idx = step % epoch_num\n",
    "\n",
    "      batch_data_list_frame1 = data_list_frame1[int(\n",
    "          batch_idx * FLAGS.batch_size): int((batch_idx + 1) * FLAGS.batch_size)]\n",
    "      batch_data_list_frame2 = data_list_frame2[int(\n",
    "          batch_idx * FLAGS.batch_size): int((batch_idx + 1) * FLAGS.batch_size)]\n",
    "      batch_data_list_frame3 = data_list_frame3[int(\n",
    "          batch_idx * FLAGS.batch_size): int((batch_idx + 1) * FLAGS.batch_size)]\n",
    "\n",
    "      # Load batch data.\n",
    "      batch_data_frame1 = np.array(\n",
    "          [dataset_frame1.process_func(line) for line in batch_data_list_frame1])\n",
    "      batch_data_frame2 = np.array(\n",
    "          [dataset_frame2.process_func(line) for line in batch_data_list_frame2])\n",
    "      batch_data_frame3 = np.array(\n",
    "          [dataset_frame3.process_func(line) for line in batch_data_list_frame3])\n",
    "\n",
    "      # batch_data_frame1 = p_queue_frame1.get_batch()\n",
    "      # batch_data_frame2 = p_queue_frame2.get_batch()\n",
    "      # batch_data_frame3 = p_queue_frame3.get_batch()\n",
    "\n",
    "      feed_dict = {input_placeholder: np.concatenate(\n",
    "          (batch_data_frame1, batch_data_frame3), 3), target_placeholder: batch_data_frame2}\n",
    "\n",
    "      # Run single step update.\n",
    "      _, loss_value = sess.run([update_op, total_loss], feed_dict=feed_dict)\n",
    "\n",
    "      if batch_idx == 0:\n",
    "        # Shuffle data at each epoch.\n",
    "        random.seed(1)\n",
    "        shuffle(data_list_frame1)\n",
    "        random.seed(1)\n",
    "        shuffle(data_list_frame2)\n",
    "        random.seed(1)\n",
    "        shuffle(data_list_frame3)\n",
    "        print('Epoch Number: %d' % int(step / epoch_num))\n",
    "\n",
    "      # Output Summary\n",
    "      if step % 10 == 0:\n",
    "        # summary_str = sess.run(summary_op, feed_dict = feed_dict)\n",
    "        # summary_writer.add_summary(summary_str, step)\n",
    "\t      print(\"Loss at step %d: %f\" % (step, loss_value))\n",
    "\n",
    "      if step % 500 == 0:\n",
    "        # Run a batch of images\n",
    "        prediction_np, target_np = sess.run(\n",
    "            [prediction, target_placeholder], feed_dict=feed_dict)\n",
    "        for i in range(0, prediction_np.shape[0]):\n",
    "          file_name = FLAGS.train_image_dir+str(i)+'_out.png'\n",
    "          file_name_label = FLAGS.train_image_dir+str(i)+'_gt.png'\n",
    "          imwrite(file_name, prediction_np[i, :, :, :])\n",
    "          imwrite(file_name_label, target_np[i, :, :, :])\n",
    "\n",
    "      # Save checkpoint\n",
    "      if step % 5000 == 0 or (step + 1) == FLAGS.max_steps:\n",
    "        checkpoint_path = os.path.join(FLAGS.train_dir, 'model.ckpt')\n",
    "        saver.save(sess, checkpoint_path, global_step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
